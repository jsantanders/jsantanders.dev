---
title: Linear Regression - Least Squares Method with Orthogonal Projection
description: This blog post delves into the application of projection techniques in linear regression, specifically the least squares method
isPublished: true
publishedAt: "2023-04-24"
tags:
  - statistics
  - linear-regression
---

## Linear regression model

Linear regression is a widely-used technique for modeling the relationship between a response variable
and one or more predictor variables. In its most general form, a regression model assumes that
the response $Y$ can be expressed as:

$$
Y = f(X_1, X_2, \cdots, X_n) + \epsilon
$$

where $\epsilon$ is the additive error term, $f$ is some unknown function and $X_1,X_2, \cdots X_n$ are
the predictors. Estimating $f$ can be quite challenging, specially in cases of multiple predictors
and limited data. This is because the function $f$ could take on any form, and it may be difficult to
identify the specific shape of the function that best describes the data.

This is where linear regression comes in. By assuming that the relationship between the predictors and
the response is linear, we can greatly simplify the modeling process. Instead of estimating the entire function $f$,
we only need to estimate the unknown parameters in the linear regression equation:

$$
Y = \beta_0 + \beta_1 X_1  + \beta_2 X_2 + \cdots + \beta_n X_n + \epsilon
$$

which is a much simpler task. Here $\beta_0, \beta_1, \beta_2, \beta_n$ are unknown parameters,
$\beta_0$ is also called the intercept or bias term. Note that the predictors themselves do not have to be linear.
For example, we can have a model of the form:

$$
Y = \beta_0 + \beta_1 log(X_1) + \beta_2 X_2^2 + \beta_3 X_3 X_1 + \epsilon
$$

which is still linear. Meanwhile:

$$
Y = \beta_0 + \beta_1 \beta_2 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon
$$

is not. Although the name implies a straight line, linear regression models can actually be quite
flexible and can handle complex datasets. To capture non-linear relationships between predictors and the response variable,
quadratic or interaction terms can be included in the model. Additionally, nonlinear predictors can be transformed
to a linear form through suitable transformations, such as logarithmic transformation, which can linearize
exponential relationships.

In practice almost all relationships could be represented or reduced to a linear form. This is why linear regression
is so widespread and popular. It is a simple yet powerful technique.

### Matrix representation

To make it easier to work with, lets present our model of a response $Y$ and $n$ predictors $X_1, X_2, \cdots, X_n$
in a tabular form:

$$
y_1     \quad x_{11} \quad x_{12} \quad \cdots \quad x_{1n} \\
y_2     \quad x_{21} \quad x_{22} \quad \cdots \quad x_{2n} \\
\vdots   \\
y_m     \quad x_{m1} \quad x_{m2} \quad \cdots \quad x_{mn}
$$

where $m$ is the number of observations or cases in the dataset. Given the actual data values, we may write the model as:

$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_n x_{in} + \epsilon_i
$$

We can write this more conveniently in matrix form as:

$$
y = X \beta + \epsilon
$$

where $y = (y_1, y_2, \cdots, y_m)^T$ is the response vector, $\epsilon = (\epsilon_1, \epsilon_2, \cdots, \epsilon_m)^T$
is the error vector, $\beta = (\beta_0, \beta_1, \cdots, \beta_n)^T$ is the parameter vector, and:

$$
X = \begin{bmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1n} \\
1 & x_{21} & x_{22} & \cdots & x_{2n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{m1} & x_{m2} & \cdots & x_{mn}
\end{bmatrix}
$$

is the design matrix. Note that the first column of $X$ is all ones. This is to account for the intercept term $\beta_0$.

### Estimating the parameters

Our goal is to estimate the parameters $\beta$ such that the model fits the data as well as possible.
Geometrically speaking it means that we want to find the vector $\beta$ such that the product
$X\beta$ is as close as possible to $y$.
